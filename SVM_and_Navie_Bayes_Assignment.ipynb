{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### SVM and Navie Bayes Assignment Q/A"
      ],
      "metadata": {
        "id": "nZs7BzSI87Ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) What is Information Gain, and how is it used in Decision Trees?\n",
        "#### > Information Gain (IG) is a key concept used in building Decision Trees for classification problems. It measures how much \"information\" or \"purity\" a feature contributes toward correctly classifying the data.It quantifies the reduction in entropy (or uncertainty) about the target variable after splitting the dataset based on a particular feature.\n",
        "#### Formulae: Information=Gain=Entropy(Parent)−∑i​ni/n​​*Entropy(Childi​)\n",
        "#### **It's Used in Decision Trees in the following ways**:\n",
        "##### For each feature:\n",
        "- Calculate the entropy of the parent dataset.\n",
        "- Split the data on that feature and calculate the weighted average entropy of the resulting subsets.\n",
        "- Compute Information Gain for that split.\n",
        "##### Choose the feature with the highest Information Gain as the best split (most reduces uncertainty).\n",
        "##### Repeat the process recursively for each branch until:\n",
        "- All samples are classified (pure nodes), or\n",
        "- A stopping criterion is met (example: max depth or min samples)."
      ],
      "metadata": {
        "id": "2pY9zFGA9BRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) What is the difference between Gini Impurity and Entropy?\n",
        "### > **Gini Impurity** : Measures how often a randomly chosen element would be incorrectly labeled if it were randomly labeled according to class distribution.\n",
        "#### **Strengths**\n",
        "- Computationally faster (no logarithms)\n",
        "-  Produces similar results to entropy\n",
        "- Works well for balanced datasets\n",
        "#### **Weaknessess**\n",
        "- Slightly less sensitive to class imbalance\n",
        "- May bias toward features with many levels\n",
        "#### **Use Cases**\n",
        "- When speed and efficiency are important (large datasets)\n",
        "- Used by CART algorithm (default in sklearn.DecisionTreeClassifier)\n",
        "#### > **Entropy** : Measures the amount of information (or uncertainty) in the dataset.\n",
        "#### **Strengths**\n",
        "- Based on information theory (clear theoretical meaning)\n",
        "- More sensitive to rare classes or uneven splits\n",
        "#### **Weaknessess**\n",
        "- Computationally slower (uses log function)\n",
        "- Adds little practical benefit over Gini in many cases\n",
        "#### **Use Cases**\n",
        "- When information gain interpretation is desired\n",
        "- Used in ID3, C4.5, and C5.0 decision tree algorithms"
      ],
      "metadata": {
        "id": "85ESInbl-yPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) What is Pre-Pruning in Decision Trees?\n",
        "#### > Pre-Pruning (also called early stopping) is a technique used in Decision Trees to stop the tree from growing too large during training before it perfectly fits (or overfits) the training data.It halting the tree's growth early if further splitting does not significantly improve model performance.Instead of letting the tree grow fully and then trimming it (as in post-pruning), we set constraints that limit its depth or size during construction.\n",
        "#### **Pre-Pruning Parameters**\n",
        "- max_depth\n",
        "- min_samples_split\n",
        "- min_samples_leaf\n",
        "- max_leaf_nodes\n",
        "- min_impurity_decrease\n",
        "#### **Advantages**\n",
        "- Prevents overfitting early\n",
        "- Reduces training time\n",
        "- Produces simpler, more interpretable trees\n",
        "#### **Disadvantages**\n",
        "- Might underfit the data if pruning is too aggressive\n",
        "- Requires careful tuning of parameters to find the right balance"
      ],
      "metadata": {
        "id": "3eGvae0BAv2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical)."
      ],
      "metadata": {
        "id": "eD-YvKaYBpzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(iris.feature_names, model.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2Sle7rdBlDP",
        "outputId": "59abdb0b-668e-497b-ca37-92eac2ec8eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n",
            "\n",
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) What is a Support Vector Machine (SVM)?\n",
        "#### > A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks most commonly for classification.It aims to find the best decision boundary (called a hyperplane) that separates data points of different classes with the maximum margin.It tries to draw a line (in 2D) or a plane (in higher dimensions) that divides the data into classes as clearly as possible.\n",
        "#### **Types of SVM**\n",
        "- Linear SVM - Works when data can be separated by a straight line (or plane).\n",
        "- Non-linear SVM - Uses kernel functions (like RBF, polynomial, sigmoid) to separate data that is not linearly separable.\n",
        "#### **Advantages**\n",
        "- Works well for high-dimensional data\n",
        "- Effective when there's a clear margin of separation\n",
        "- Robust to overfitting (especially with regularization)\n",
        "#### **Disadvantages**\n",
        "- Not ideal for very large datasets (computationally expensive)\n",
        "- Performance drops when classes overlap heavily\n",
        "- Requires tuning of kernel and regularization parameters"
      ],
      "metadata": {
        "id": "BxsmwWrHCTfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) What is the Kernel Trick in SVM?\n",
        "#### > The Kernel Trick in Support Vector Machines (SVMs) is a mathematical technique that allows the algorithm to classify data that isn't linearly separable by transforming it into a higher-dimensional space without actually computing the transformation explicitly.\n",
        "#### **Advantages**\n",
        "- Makes SVMs powerful for non-linear classification\n",
        "- Avoids the computational cost of explicitly transforming data\n",
        "- Works well with complex boundaries\n",
        "#### **Disadvantages**\n",
        "- Computationally expensive for large datasets\n",
        "- Hard to choose the right kernel and parameters\n",
        "- Poor interpretability"
      ],
      "metadata": {
        "id": "qNOQo4CKDHFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7) Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n"
      ],
      "metadata": {
        "id": "cABXQcofDugj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test_scaled)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"SVM with Linear kernel Accuracy: {accuracy_linear:.2f}\")\n",
        "print(f\"SVM with RBF kernel Accuracy: {accuracy_rbf:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLfYsTezDyJ0",
        "outputId": "025a24fa-b704-4a87-dfe6-513b7caf0173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM with Linear kernel Accuracy: 0.96\n",
            "SVM with RBF kernel Accuracy: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8) What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "#### > The Naïve Bayes (NB) classifier is a probabilistic machine learning algorithm based on Bayes Theorem. It is used for classification tasks, especially text classification, spam detection, and sentiment analysis.\n",
        "#### Bayes Theorem formula:P(C/X)=P(X)P(X/C)⋅P(C)​\n",
        "#### Where:\n",
        "- P(C/X) - probability of class C given features X (posterior)\n",
        "- P(X/C) - probability of features given class C (likelihood)\n",
        "- P(C)- prior probability of class C\n",
        "- P(X) - probability of features X (evidence)\n",
        "##### The classifier predicts the class C that maximizes P(C/X)\n",
        "#### **It is called “Naïve” because it makes a strong assumption** : All features are independent of each other given the class.In reality, features are often correlated, but Naïve Bayes ignores these dependencies. Despite this simplification, it often works surprisingly well in practice.\n",
        "#### **Characteristics**\n",
        "- Simple and fast to train.\n",
        "- Performs well with high-dimensional data.\n",
        "#### **Common variants**\n",
        "- Gaussian Naïve Bayes - for continuous features\n",
        "- Multinomial Naïve Bayes - for count-based features (text)\n",
        "- Bernoulli Naïve Bayes - for binary features\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DrUkrb8xEI7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9) Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "#### > **Gaussian Naïve Bayes** : Gaussian Naïve Bayes (GNB) is a type of Naïve Bayes classifier that assumes the features (predictor variables) follow a normal (Gaussian) distribution. It is commonly used for continuous data.\n",
        "#### **When to Use**\n",
        "- The features are continuous.\n",
        "- The features approximately follow a normal distribution (Example: height, weight, test scores, etc.).\n",
        "#### **Advantages**\n",
        "- Simple and fast to train.\n",
        "- Works well with small datasets.\n",
        "- Performs surprisingly well even with the independence assumption.\n",
        "#### **Disadvantages**\n",
        "- Assumes normality — not suitable for non-Gaussian data.\n",
        "- Assumes features are independent (which is rarely true).\n",
        "- Sensitive to outliers.\n",
        "#### **Multinomial Naïve Bayes** : Multinomial Naïve Bayes (MNB) is a type of Naïve Bayes classifier designed for discrete (count-based) data, such as word counts or term frequencies in text classification tasks.It's one of the most popular algorithms for document classification for example, spam detection, sentiment analysis, and news categorization.\n",
        "#### **When to Use**\n",
        "- Your features represent counts (Example: term frequency vectors).\n",
        "- Data is non-negative integers (Example:  number of occurrences).\n",
        "#### **Advantages**\n",
        "- Works very well for text data (Example:  word counts).\n",
        "- Simple and computationally efficient.\n",
        "- Performs well with large feature spaces (like vocabulary).\n",
        "#### **Disadvantages**\n",
        "- Assumes feature independence.\n",
        "- Doesn't perform well with continuous data.\n",
        "- Requires non-negative feature values.\n",
        "#### **Bernoulli Naïve Bayes** : Bernoulli Naïve Bayes (BNB) is a variant of the Naïve Bayes classifier used for binary/boolean features where each feature can take only two values: 1 (present) or 0 (absent).It is commonly used in text classification, especially when we care only about whether a word appears in a document, not how many times it appears.\n",
        "#### **When to Use**\n",
        "- Features are binary (Example:  yes/no, present/absent).\n",
        "- You're working with text data represented as binary word presence vectors.\n",
        "- You want the model to penalize missing words as much as extra ones.\n",
        "#### **Advantages**\n",
        "- Works well with binary features (like word presence).\n",
        "- Efficient and easy to implement.\n",
        "- Good for high-dimensional data (Example:  text).\n",
        "#### **Disadvantages**\n",
        "- Not suitable for continuous or count-based data.\n",
        "- Assumes features are independent.\n",
        "- May perform worse than Multinomial NB when word frequency matters."
      ],
      "metadata": {
        "id": "rjRq5-GHFuVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10) Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy"
      ],
      "metadata": {
        "id": "59enDuT-GDpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = GaussianNB()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Gaussian Naïve Bayes:\", accuracy)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkO6dojMGHvl",
        "outputId": "333fcfef-afe5-4834-9bb0-818da118f8b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes: 0.9415204678362573\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.90      0.92        63\n",
            "           1       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    }
  ]
}